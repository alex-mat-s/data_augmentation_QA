{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation with LLM \n",
    "\n",
    "### Plan:\n",
    "\n",
    "1. Make summaries for answers\n",
    "\n",
    "2. Divide the context into chunks, after that generate question and answer\n",
    "----\n",
    "\n",
    "Models used: \n",
    "\n",
    "- Question generation: ```mrm8488/t5-base-finetuned-question-generation-ap```\n",
    "- Answer generation: ```valhalla/t5-base-qa-qg-hl```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./MedQuAD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1. Answers augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate summaries\n",
    "def generate_summary(text):\n",
    "    input_ids = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=2000, truncation=True)\n",
    "    summary_ids = model.generate(input_ids, max_length=500, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Apply the function to create a new 'summary' column\n",
    "df['answer_summary'] = df['answer'].apply(generate_summary)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df[['context', 'answer', 'question', 'answer_summary']])\n",
    "\n",
    "df.to_csv('./MedQuAD_with_augmented_answers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2. Generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model = model.to(device)\n",
    "\n",
    "def generate_question(context, max_length=64):\n",
    "    input_text = \"context: %s </s>\" % context\n",
    "    features = tokenizer([input_text], return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate answer\n",
    "    answer_output = model.generate(input_ids=features['input_ids'],\n",
    "                                   attention_mask=features['attention_mask'],\n",
    "                                   max_length=max_length)\n",
    "\n",
    "    # Extract question\n",
    "    question = tokenizer.decode(answer_output[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  # Initialize an empty list to store the results\n",
    "max_length = 512  # Maximum sequence length supported by the model\n",
    "\n",
    "# Iterate over the DataFrame and generate questions for each chunk of the context\n",
    "for index, context in enumerate(df['context'].unique()):\n",
    "    j = 0\n",
    "    # Split the context into chunks of 512 tokens\n",
    "    context_chunks = [context[i:i + max_length] for i in range(0, len(context), max_length)]\n",
    "\n",
    "    # Generate answer and question for each chunk\n",
    "    for chunk in context_chunks:\n",
    "        generated_question = generate_question(chunk)\n",
    "\n",
    "        # Append the results to the data list along with the index\n",
    "        data.append((chunk, generated_question, index))\n",
    "        j += 1\n",
    "\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "columns = ['context_chunk', 'generated_question']\n",
    "result_df = pd.DataFrame(data, columns=columns)\n",
    "result_df.to_csv('./questions_generated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2. Generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"valhalla/t5-base-qa-qg-hl\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Assuming your dataset is stored in a pandas DataFrame named 'df'\n",
    "# with columns: 'context_chunk' and 'generated_question'\n",
    "\n",
    "# Function to generate answers\n",
    "def generate_answer(context_chunk, generated_question):\n",
    "    input_text = f\"answer: {context_chunk} context: {generated_question}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    answer_ids = model.generate(input_ids, max_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Apply the function to create a new 'generated_answer' column\n",
    "result_df['generated_answer'] = result_df.apply(lambda row: generate_answer(row['context_chunk'], row['generated_question']), axis=1)\n",
    "\n",
    "result_df.to_csv('./MedQuAD_generated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate into russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate(s):\n",
    "  return GoogleTranslator('en', 'ru').translate(s)\n",
    "\n",
    "# translate df with augmented answers \n",
    "df['context'] = df.context.apply(translate)\n",
    "df['answer_summary'] = df.answer_summary.apply(translate)\n",
    "df['question'] = df.question.apply(translate)\n",
    "\n",
    "# translate df with generated answers and questions\n",
    "result_df['context_chunk'] = result_df.context_chunk.apply(translate)\n",
    "result_df['generated_question'] = result_df.generated_question.apply(translate)\n",
    "result_df['generated_answer'] = result_df.generated_answer.apply(translate)\n",
    "\n",
    "\n",
    "result_df.to_csv('./MedQuAD_qa_generated_russian.csv')\n",
    "df.to_csv('./MedQuAD_answers_aigmented_russian.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
